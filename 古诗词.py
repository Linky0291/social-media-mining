import time
import csv
import os
import random
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.options import Options

def scrape_gushiwen_multipage():
    # ==========================================
    # ğŸ‘‡ ä¿å­˜è·¯å¾„
    save_path = "D:/python_Lin/çˆ¬è™«å­¦ä¹ å®ä¹ /çˆ¬è™«/gushiwen_5_pages.csv"
    # ==========================================

    print("ğŸš€ å¯åŠ¨æ‰¹é‡çˆ¬è™«ï¼Œç›®æ ‡ï¼šå¤è¯—æ–‡ç½‘ (å‰ 5 é¡µ)...")

    # è®¾ç½®æµè§ˆå™¨
    chrome_options = Options()
    chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
    # chrome_options.add_argument("--headless") # æƒ³çœ‹è¿‡ç¨‹å°±ä¸è¦å–æ¶ˆæ³¨é‡Š

    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    
    all_data = [] # ç”¨æ¥å­˜æ”¾æ‰€æœ‰é¡µé¢çš„æ•°æ®

    try:
        # ==========================================
        # ğŸ”„ å¾ªç¯å¼€å§‹ï¼šä»ç¬¬ 1 é¡µçˆ¬åˆ°ç¬¬ 5 é¡µ
        # ==========================================
        for page in range(1, 6): 
            print(f"\nğŸ“„ æ­£åœ¨è¯»å–ç¬¬ {page} é¡µ...")
            
            # æ„é€ åŠ¨æ€ URL
            url = f"https://www.gushiwen.cn/default_{page}.aspx"
            driver.get(url)
            
            # éšæœºç­‰å¾… 2-4 ç§’ï¼Œé˜²æ­¢ç¿»é¡µå¤ªå¿«è¢«å° IP
            time.sleep(random.uniform(2, 4))

            # å®šä½è¯—è¯å¡ç‰‡
            poem_cards = driver.find_elements(By.CSS_SELECTOR, ".left .sons")
            
            current_page_count = 0

            for card in poem_cards:
                try:
                    # æå–æ ‡é¢˜ (è¿‡æ»¤å¹¿å‘Š)
                    try:
                        title = card.find_element(By.CSS_SELECTOR, "b").text
                    except:
                        continue

                    # æå–ä½œè€…
                    source_text = card.find_element(By.CSS_SELECTOR, ".source").text
                    
                    # æå–å†…å®¹
                    content = card.find_element(By.CSS_SELECTOR, ".contson").text.replace('\n', ' ')

                    # å­˜å…¥å¤§åˆ—è¡¨
                    all_data.append([title, source_text, content])
                    current_page_count += 1
                    
                    # æ‰“å°ä¸€æ¡ç®€ç•¥ä¿¡æ¯è¯æ˜æ´»ç€
                    print(f"  æŠ“å–: {title} ({source_text})")

                except Exception:
                    continue
            
            print(f"  âœ… ç¬¬ {page} é¡µå®Œæˆï¼Œæœ¬é¡µè·å– {current_page_count} é¦–ã€‚")

        # ==========================================
        # ğŸ’¾ æ‰€æœ‰é¡µé¢çˆ¬å®Œåï¼Œç»Ÿä¸€ä¿å­˜
        # ==========================================
        print("-" * 60)
        print("ğŸ’¾ æ­£åœ¨ä¿å­˜æ‰€æœ‰æ•°æ®...")
        
        os.makedirs(os.path.dirname(save_path), exist_ok=True)

        with open(save_path, mode='w', newline='', encoding='utf-8-sig') as file:
            writer = csv.writer(file)
            writer.writerow(["æ ‡é¢˜", "æœä»£/ä½œè€…", "æ­£æ–‡"])
            writer.writerows(all_data)

        print(f"ğŸ‰ å¤§åŠŸå‘Šæˆï¼å…±çˆ¬å– {len(all_data)} é¦–è¯—è¯ã€‚")
        print(f"ğŸ“„ æ–‡ä»¶è·¯å¾„: {save_path}")

    except Exception as e:
        print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")

    finally:
        driver.quit()

if __name__ == "__main__":
    scrape_gushiwen_multipage()